{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Microservices Workload Prediction Experiment\n",
        "\n",
        "This notebook demonstrates workload prediction using the Alibaba Microservices Trace v2022 dataset.\n",
        "\n",
        "## Contents\n",
        "1. Data Loading and Exploration\n",
        "2. Data Analysis and Visualization\n",
        "3. Data Preprocessing\n",
        "4. Model Training and Evaluation\n",
        "5. Results Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Environment\n",
        "\n",
        "nvidia rtx 5090\n",
        "cuda 13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Add project path\n",
        "project_path = '/root/repository/clusterdata/cluster-trace-microservices-v2022/workload_prediction'\n",
        "if project_path not in sys.path:\n",
        "    sys.path.insert(0, project_path)\n",
        "\n",
        "# Import project modules\n",
        "from config import DATA_CONFIG, MODEL_CONFIG, EXPERIMENT_CONFIG, create_output_dirs\n",
        "from data_loader import DataModule, MSMetricsLoader, MSRTMCRLoader\n",
        "from data_analysis import WorkloadAnalyzer, WorkloadVisualizer, analyze_dataset\n",
        "from models import (\n",
        "    create_model, get_available_models, get_model_info,\n",
        "    LSTMPredictor, GRUPredictor, TransformerPredictor,\n",
        "    AttentionLSTM, TCNPredictor, NLinear, DLinear,\n",
        "    PatchTST, Informer, TimesNet, Autoformer\n",
        ")\n",
        "from trainer import (\n",
        "    WorkloadTrainer, \n",
        "    run_deep_learning_experiment,\n",
        "    run_baseline_experiment,\n",
        "    compare_all_models,\n",
        "    compute_metrics,\n",
        "    print_metrics,\n",
        "    plot_training_history,\n",
        "    plot_predictions,\n",
        "    plot_model_comparison,\n",
        "    plot_comprehensive_comparison,\n",
        "    generate_latex_table\n",
        ")\n",
        "from baseline_models import get_all_baseline_models, compare_baselines\n",
        "\n",
        "# Create output directories\n",
        "create_output_dirs()\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print('Setup complete!')\n",
        "print(f'Data root: {DATA_CONFIG.data_root}')\n",
        "print(f'\\nAvailable deep learning models: {get_available_models()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data module\n",
        "dm = DataModule()\n",
        "\n",
        "# Load MSMetrics data (adjust num_files based on your data)\n",
        "# Each file covers 30 minutes, so:\n",
        "#   4 files = 2 hours, 24 files = 12 hours (full dataset)\n",
        "NUM_MSMETRICS_FILES = 24  # Use all available data for better results\n",
        "LOAD_MSRTMCR = False      # Set True to load call rate/response time features\n",
        "\n",
        "print('Loading data...')\n",
        "dm.load_data(\n",
        "    num_msmetrics_files=NUM_MSMETRICS_FILES,\n",
        "    load_msrtmcr=LOAD_MSRTMCR,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the loaded data\n",
        "print('\\nData shape:', dm.msmetrics_data.shape)\n",
        "print('\\nColumn names:', dm.msmetrics_data.columns.tolist())\n",
        "print('\\nData types:')\n",
        "print(dm.msmetrics_data.dtypes)\n",
        "print('\\nFirst few rows:')\n",
        "dm.msmetrics_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print('Basic statistics:')\n",
        "dm.msmetrics_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize analyzer and visualizer with optimized settings\n",
        "analyzer = WorkloadAnalyzer(sample_size=1_000_000)  # Limit sample size for speed\n",
        "visualizer = WorkloadVisualizer()\n",
        "\n",
        "# Run comprehensive analysis with FAST MODE (uses sampling for large datasets)\n",
        "stats, service_stats, temporal = analyze_dataset(\n",
        "    dm.msmetrics_data, \n",
        "    fast_mode=True,      # Use optimized analysis with sampling\n",
        "    sample_size=1_000_000  # Sample size for visualization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top microservices by data volume\n",
        "top_services = dm.get_top_services(20)\n",
        "print('Top 20 microservices by data points:')\n",
        "for i, (svc, count) in enumerate(top_services, 1):\n",
        "    print(f'  {i}. {svc}: {count:,} records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distribution of CPU and memory utilization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=120)\n",
        "\n",
        "# CPU utilization distribution\n",
        "axes[0].hist(dm.msmetrics_data['cpu_utilization'].dropna(), bins=50, \n",
        "             density=True, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
        "axes[0].set_xlabel('CPU Utilization')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Distribution of CPU Utilization')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory utilization distribution\n",
        "axes[1].hist(dm.msmetrics_data['memory_utilization'].dropna(), bins=50,\n",
        "             density=True, alpha=0.7, edgecolor='black', linewidth=0.5, color='orange')\n",
        "axes[1].set_xlabel('Memory Utilization')\n",
        "axes[1].set_ylabel('Density')\n",
        "axes[1].set_title('Distribution of Memory Utilization')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(EXPERIMENT_CONFIG.figures_path, 'utilization_distribution.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Data for Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a microservice for prediction\n",
        "# Choose one with sufficient data points\n",
        "MIN_DATA_POINTS = 200\n",
        "\n",
        "# Find services with enough data\n",
        "service_candidates = []\n",
        "for svc, count in top_services:\n",
        "    if count >= MIN_DATA_POINTS:\n",
        "        service_candidates.append((svc, count))\n",
        "\n",
        "print(f'Found {len(service_candidates)} services with >= {MIN_DATA_POINTS} data points')\n",
        "\n",
        "if service_candidates:\n",
        "    # Select the service with most data\n",
        "    selected_service = service_candidates[0][0]\n",
        "    print(f'\\nSelected service for prediction: {selected_service}')\n",
        "    print(f'Data points: {service_candidates[0][1]:,}')\n",
        "else:\n",
        "    print('No suitable service found. Please load more data.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare time series data for the selected service\n",
        "if 'selected_service' in dir():\n",
        "    ts_data = dm.prepare_service_data(\n",
        "        selected_service,\n",
        "        features=['cpu_utilization', 'memory_utilization'],\n",
        "        normalize=True,\n",
        "        add_features=False  # Set to True to add lag/rolling features\n",
        "    )\n",
        "    \n",
        "    if ts_data is not None:\n",
        "        print(f'Time series shape: {ts_data.shape}')\n",
        "        print(f'Features: {ts_data.columns.tolist()}')\n",
        "        print(f'\\nTime series statistics:')\n",
        "        print(ts_data.describe())\n",
        "    else:\n",
        "        print('Failed to prepare time series data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "if 'ts_data' in dir() and ts_data is not None:\n",
        "    print('Creating DataLoaders...')\n",
        "    print(f'Sequence length: {MODEL_CONFIG.seq_length}')\n",
        "    print(f'Prediction horizon: {MODEL_CONFIG.pred_length}')\n",
        "    print(f'Batch size: {MODEL_CONFIG.batch_size}')\n",
        "    \n",
        "    train_loader, val_loader, test_loader = dm.create_dataloaders(\n",
        "        ts_data.values,\n",
        "        target_idx=0  # Predict CPU utilization\n",
        "    )\n",
        "    \n",
        "    # Verify data shapes\n",
        "    for x, y in train_loader:\n",
        "        print(f'\\nBatch shapes:')\n",
        "        print(f'  Input (X): {x.shape}')\n",
        "        print(f'  Target (Y): {y.shape}')\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for GPU availability and compatibility\n",
        "import torch\n",
        "\n",
        "def check_gpu_compatibility():\n",
        "    \"\"\"Check if GPU is available and compatible with PyTorch.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"GPU COMPATIBILITY CHECK\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        \n",
        "        # Test if CUDA actually works\n",
        "        try:\n",
        "            test_tensor = torch.zeros(1).cuda()\n",
        "            test_result = test_tensor + 1\n",
        "            del test_tensor, test_result\n",
        "            print(\"\\n✓ CUDA test passed! GPU can be used.\")\n",
        "            return 'cuda'\n",
        "        except RuntimeError as e:\n",
        "            print(f\"\\n✗ CUDA test failed: {e}\")\n",
        "            print(\"\\nNote: Your GPU (RTX 5090) uses Blackwell architecture which may\")\n",
        "            print(\"require PyTorch 2.5+ with CUDA 13 support. Using CPU instead.\")\n",
        "            return 'cpu'\n",
        "    else:\n",
        "        print(\"\\nNo CUDA-capable GPU detected. Using CPU.\")\n",
        "        return 'cpu'\n",
        "\n",
        "# Run compatibility check\n",
        "recommended_device = check_gpu_compatibility()\n",
        "print(f\"\\nRecommended device: {recommended_device}\")\n",
        "\n",
        "# Set device for this notebook\n",
        "device = torch.device(recommended_device)\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAIN ALL DEEP LEARNING MODELS\n",
        "# ============================================================\n",
        "# This section trains all 11 available models for comprehensive comparison\n",
        "\n",
        "if 'train_loader' in dir():\n",
        "    input_size = ts_data.shape[1]  # Number of features\n",
        "    output_size = MODEL_CONFIG.pred_length\n",
        "    seq_length = MODEL_CONFIG.seq_length\n",
        "    \n",
        "    print(f'Input size: {input_size}')\n",
        "    print(f'Output size: {output_size}')\n",
        "    print(f'Sequence length: {seq_length}')\n",
        "    \n",
        "    # Training configuration\n",
        "    NUM_EPOCHS = 30  # Adjust as needed\n",
        "    \n",
        "    # Get all available models\n",
        "    all_model_types = get_available_models()\n",
        "    print(f'\\nModels to train: {all_model_types}')\n",
        "    print(f'Total: {len(all_model_types)} models')\n",
        "    \n",
        "    # Store all results\n",
        "    all_dl_results = {}\n",
        "    all_dl_trainers = {}\n",
        "    \n",
        "    # Train each model\n",
        "    for model_type in all_model_types:\n",
        "        print(f'\\n{\"=\"*60}')\n",
        "        print(f'Training {model_type.upper()}...')\n",
        "        print(f'{\"=\"*60}')\n",
        "        \n",
        "        try:\n",
        "            results, trainer = run_deep_learning_experiment(\n",
        "                model_type=model_type,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                test_loader=test_loader,\n",
        "                input_size=input_size,\n",
        "                output_size=output_size,\n",
        "                seq_length=seq_length,\n",
        "                num_epochs=NUM_EPOCHS\n",
        "            )\n",
        "            \n",
        "            all_dl_results[model_type] = results\n",
        "            all_dl_trainers[model_type] = trainer\n",
        "            \n",
        "            print(f'\\n✓ {model_type.upper()} completed successfully')\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f'\\n✗ {model_type.upper()} failed: {e}')\n",
        "            all_dl_results[model_type] = None\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'TRAINING COMPLETE')\n",
        "    print(f'{\"=\"*60}')\n",
        "    print(f'Successfully trained: {sum(1 for v in all_dl_results.values() if v is not None)}/{len(all_model_types)} models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TRAIN BASELINE MODELS\n",
        "# ============================================================\n",
        "# Compare with traditional ML baselines (Random Forest, XGBoost, etc.)\n",
        "\n",
        "if 'train_loader' in dir():\n",
        "    print('='*60)\n",
        "    print('TRAINING BASELINE MODELS')\n",
        "    print('='*60)\n",
        "    \n",
        "    # Extract numpy arrays from DataLoaders for baseline models\n",
        "    X_train_list, y_train_list = [], []\n",
        "    X_test_list, y_test_list = [], []\n",
        "    \n",
        "    for x, y in train_loader:\n",
        "        X_train_list.append(x.numpy())\n",
        "        y_train_list.append(y.numpy())\n",
        "    \n",
        "    if test_loader:\n",
        "        for x, y in test_loader:\n",
        "            X_test_list.append(x.numpy())\n",
        "            y_test_list.append(y.numpy())\n",
        "    \n",
        "    X_train = np.concatenate(X_train_list)\n",
        "    y_train = np.concatenate(y_train_list)\n",
        "    X_test = np.concatenate(X_test_list)\n",
        "    y_test = np.concatenate(y_test_list)\n",
        "    \n",
        "    print(f'Training data shape: X={X_train.shape}, y={y_train.shape}')\n",
        "    print(f'Test data shape: X={X_test.shape}, y={y_test.shape}')\n",
        "    \n",
        "    # Run baseline experiments\n",
        "    baseline_results_df = run_baseline_experiment(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        include_ml=True,  # Include ML models (XGBoost, LightGBM, Random Forest)\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    print('\\n' + '='*60)\n",
        "    print('BASELINE RESULTS')\n",
        "    print('='*60)\n",
        "    print(baseline_results_df.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMBINE ALL RESULTS FOR COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "if 'all_dl_results' in dir() and 'baseline_results_df' in dir():\n",
        "    # Combine deep learning results into a DataFrame\n",
        "    dl_results_list = []\n",
        "    \n",
        "    for model_type, results in all_dl_results.items():\n",
        "        if results is not None:\n",
        "            dl_results_list.append({\n",
        "                'model': model_type.upper(),\n",
        "                'model_class': 'Deep Learning',\n",
        "                'mse': results['eval_metrics'].get('mse', float('inf')),\n",
        "                'rmse': results['eval_metrics'].get('rmse', float('inf')),\n",
        "                'mae': results['eval_metrics'].get('mae', float('inf')),\n",
        "                'mape': results['eval_metrics'].get('mape', float('inf')),\n",
        "                'smape': results['eval_metrics'].get('smape', float('inf')),\n",
        "                'r2': results['eval_metrics'].get('r2', float('-inf')),\n",
        "                'training_time': results['training_time'],\n",
        "                'num_params': results['num_params']\n",
        "            })\n",
        "    \n",
        "    dl_results_df = pd.DataFrame(dl_results_list)\n",
        "    \n",
        "    # Add model_class to baseline results\n",
        "    baseline_results_df['model_class'] = 'Baseline'\n",
        "    baseline_results_df['num_params'] = 0\n",
        "    baseline_results_df = baseline_results_df.rename(columns={'model': 'model'})\n",
        "    \n",
        "    # Combine all results\n",
        "    all_results_df = pd.concat([\n",
        "        dl_results_df,\n",
        "        baseline_results_df[['model', 'model_class', 'mse', 'rmse', 'mae', 'mape', 'smape', 'r2', 'training_time', 'num_params']]\n",
        "    ], ignore_index=True)\n",
        "    \n",
        "    # Sort by RMSE\n",
        "    all_results_df = all_results_df.sort_values('rmse').reset_index(drop=True)\n",
        "    \n",
        "    print('='*80)\n",
        "    print('COMPREHENSIVE MODEL COMPARISON (Sorted by RMSE)')\n",
        "    print('='*80)\n",
        "    print(all_results_df.to_string())\n",
        "    \n",
        "    # Save to CSV\n",
        "    all_results_df.to_csv(\n",
        "        os.path.join(EXPERIMENT_CONFIG.results_path, 'all_models_comparison.csv'),\n",
        "        index=False\n",
        "    )\n",
        "    print(f'\\nResults saved to: {EXPERIMENT_CONFIG.results_path}/all_models_comparison.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZE COMPARISON RESULTS\n",
        "# ============================================================\n",
        "\n",
        "if 'all_results_df' in dir() and len(all_results_df) > 0:\n",
        "    # Rename columns for visualization\n",
        "    viz_df = all_results_df.rename(columns={'model': 'model_type'})\n",
        "    \n",
        "    # 1. Bar chart comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12), dpi=120)\n",
        "    \n",
        "    # RMSE comparison\n",
        "    sorted_df = viz_df.sort_values('rmse')\n",
        "    colors = ['#2ecc71' if c == 'Baseline' else '#3498db' for c in sorted_df['model_class']]\n",
        "    axes[0, 0].barh(sorted_df['model_type'], sorted_df['rmse'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    axes[0, 0].set_xlabel('RMSE (lower is better)')\n",
        "    axes[0, 0].set_title('Model Comparison by RMSE')\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # MAE comparison\n",
        "    sorted_df = viz_df.sort_values('mae')\n",
        "    colors = ['#2ecc71' if c == 'Baseline' else '#3498db' for c in sorted_df['model_class']]\n",
        "    axes[0, 1].barh(sorted_df['model_type'], sorted_df['mae'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    axes[0, 1].set_xlabel('MAE (lower is better)')\n",
        "    axes[0, 1].set_title('Model Comparison by MAE')\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # R² comparison\n",
        "    sorted_df = viz_df.sort_values('r2', ascending=False)\n",
        "    colors = ['#2ecc71' if c == 'Baseline' else '#3498db' for c in sorted_df['model_class']]\n",
        "    axes[1, 0].barh(sorted_df['model_type'], sorted_df['r2'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    axes[1, 0].set_xlabel('R² (higher is better)')\n",
        "    axes[1, 0].set_title('Model Comparison by R²')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Training time comparison\n",
        "    sorted_df = viz_df.sort_values('training_time')\n",
        "    colors = ['#2ecc71' if c == 'Baseline' else '#3498db' for c in sorted_df['model_class']]\n",
        "    axes[1, 1].barh(sorted_df['model_type'], sorted_df['training_time'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    axes[1, 1].set_xlabel('Training Time (seconds)')\n",
        "    axes[1, 1].set_title('Training Time Comparison')\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor='#2ecc71', label='Baseline'), Patch(facecolor='#3498db', label='Deep Learning')]\n",
        "    fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.02))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(EXPERIMENT_CONFIG.figures_path, 'model_comparison_comprehensive.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Generate comprehensive comparison plots\n",
        "    print('\\nGenerating comprehensive comparison plots...')\n",
        "    plot_comprehensive_comparison(viz_df, save_dir=EXPERIMENT_CONFIG.figures_path)\n",
        "    \n",
        "    print(f'\\nAll figures saved to: {EXPERIMENT_CONFIG.figures_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXPERIMENT SUMMARY\n",
        "# ============================================================\n",
        "\n",
        "print('='*80)\n",
        "print('EXPERIMENT SUMMARY')\n",
        "print('=' * 60)\n",
        "\n",
        "print('='*80)\n",
        "\n",
        "print(f'\\nDataset: Alibaba Microservices Trace v2022')\n",
        "print(f'   Target: {EXPERIMENT_CONFIG.target_variable}')\n",
        "print(f'   Service: {selected_service}')\n",
        "print(f'   Sequence length: {MODEL_CONFIG.seq_length} time steps')\n",
        "print(f'   Prediction horizon: {MODEL_CONFIG.pred_length} time steps')\n",
        "\n",
        "if 'all_results_df' in dir() and len(all_results_df) > 0:\n",
        "    # Overall best model\n",
        "    best_overall = all_results_df.loc[all_results_df['rmse'].idxmin()]\n",
        "    \n",
        "    print(f'\\nBEST OVERALL MODEL: {best_overall[\"model\"]}')\n",
        "    print(f'   - RMSE:  {best_overall[\"rmse\"]:.6f}')\n",
        "    print(f'   - MAE:   {best_overall[\"mae\"]:.6f}')\n",
        "    print(f'   - R²:    {best_overall[\"r2\"]:.6f}')\n",
        "    print(f'   - MAPE:  {best_overall[\"mape\"]:.2f}%')\n",
        "    \n",
        "    # Best deep learning model\n",
        "    dl_results = all_results_df[all_results_df['model_class'] == 'Deep Learning']\n",
        "    if len(dl_results) > 0:\n",
        "        best_dl = dl_results.loc[dl_results['rmse'].idxmin()]\n",
        "        print(f'\\nBEST DEEP LEARNING MODEL: {best_dl[\"model\"]}')\n",
        "        print(f'   - RMSE:  {best_dl[\"rmse\"]:.6f}')\n",
        "        print(f'   - MAE:   {best_dl[\"mae\"]:.6f}')\n",
        "        print(f'   - R²:    {best_dl[\"r2\"]:.6f}')\n",
        "    \n",
        "    # Best baseline model\n",
        "    baseline_results = all_results_df[all_results_df['model_class'] == 'Baseline']\n",
        "    if len(baseline_results) > 0:\n",
        "        best_baseline = baseline_results.loc[baseline_results['rmse'].idxmin()]\n",
        "        print(f'\\nBEST BASELINE MODEL: {best_baseline[\"model\"]}')\n",
        "        print(f'   - RMSE:  {best_baseline[\"rmse\"]:.6f}')\n",
        "        print(f'   - MAE:   {best_baseline[\"mae\"]:.6f}')\n",
        "        print(f'   - R²:    {best_baseline[\"r2\"]:.6f}')\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(f'\\nSUMMARY STATISTICS:')\n",
        "    print(f'   Total models evaluated: {len(all_results_df)}')\n",
        "    print(f'   Deep learning models: {len(dl_results)}')\n",
        "    print(f'   Baseline models: {len(baseline_results)}')\n",
        "    \n",
        "    # Top 5 models\n",
        "    print(f'\\nTOP 5 MODELS (by RMSE):')\n",
        "    top5 = all_results_df.nsmallest(5, 'rmse')\n",
        "    for i, (_, row) in enumerate(top5.iterrows(), 1):\n",
        "        print(f'   {i}. {row[\"model\"]}: RMSE={row[\"rmse\"]:.6f}, R²={row[\"r2\"]:.6f}')\n",
        "\n",
        "print(f'\\nOUTPUT FILES:')\n",
        "print(f'   Results: {EXPERIMENT_CONFIG.results_path}/all_models_comparison.csv')\n",
        "print(f'   Figures: {EXPERIMENT_CONFIG.figures_path}/')\n",
        "\n",
        "# Generate LaTeX table for paper\n",
        "if 'all_results_df' in dir():\n",
        "    print(f'\\nLaTeX Table for Paper:')\n",
        "    print('-'*60)\n",
        "    latex = generate_latex_table(all_results_df.rename(columns={'model': 'model_type'}))\n",
        "    print(latex)\n",
        "    \n",
        "    # Save LaTeX to file\n",
        "    with open(os.path.join(EXPERIMENT_CONFIG.results_path, 'results_table.tex'), 'w') as f:\n",
        "        f.write(latex)\n",
        "    print(f'\\nLaTeX table saved to: {EXPERIMENT_CONFIG.results_path}/results_table.tex')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VISUALIZE BEST MODEL PREDICTIONS\n",
        "# ============================================================\n",
        "\n",
        "if 'all_results_df' in dir() and 'all_dl_trainers' in dir():\n",
        "    # Get best deep learning model\n",
        "    dl_results = all_results_df[all_results_df['model_class'] == 'Deep Learning']\n",
        "    if len(dl_results) > 0:\n",
        "        best_model_name = dl_results.loc[dl_results['rmse'].idxmin(), 'model'].lower()\n",
        "        \n",
        "        if best_model_name in all_dl_trainers and all_dl_trainers[best_model_name] is not None:\n",
        "            best_trainer = all_dl_trainers[best_model_name]\n",
        "            \n",
        "            # Get predictions\n",
        "            predictions, targets = best_trainer.predict(test_loader)\n",
        "            \n",
        "            # Plot predictions\n",
        "            fig, axes = plt.subplots(2, 1, figsize=(14, 10), dpi=120)\n",
        "            \n",
        "            # Time series comparison\n",
        "            n_points = min(100, len(predictions))\n",
        "            x = range(n_points)\n",
        "            \n",
        "            axes[0].plot(x, targets[:n_points], 'b-', linewidth=1.5, label='Actual', alpha=0.8)\n",
        "            axes[0].plot(x, predictions[:n_points], 'r--', linewidth=1.5, label='Predicted', alpha=0.8)\n",
        "            axes[0].fill_between(x, targets[:n_points], predictions[:n_points], alpha=0.2, color='gray')\n",
        "            axes[0].set_xlabel('Time Step')\n",
        "            axes[0].set_ylabel('CPU Utilization (normalized)')\n",
        "            axes[0].set_title(f'Best Model ({best_model_name.upper()}) Predictions vs Actual')\n",
        "            axes[0].legend()\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Scatter plot\n",
        "            axes[1].scatter(targets, predictions, alpha=0.5, s=30)\n",
        "            min_val, max_val = min(targets.min(), predictions.min()), max(targets.max(), predictions.max())\n",
        "            axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
        "            axes[1].set_xlabel('Actual')\n",
        "            axes[1].set_ylabel('Predicted')\n",
        "            axes[1].set_title('Prediction Scatter Plot')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(EXPERIMENT_CONFIG.figures_path, 'best_model_predictions.png'), dpi=150)\n",
        "            plt.show()\n",
        "            \n",
        "            print(f'\\nPrediction visualization saved to: {EXPERIMENT_CONFIG.figures_path}/best_model_predictions.png')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "machinelearning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
